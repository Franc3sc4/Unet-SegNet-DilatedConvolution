{"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [code]\nimport os\nimport cv2\nimport numpy as np\nimport albumentations as album\nfrom tensorflow import keras\nfrom patchify import patchify\n\ndef common_images(buildings_paths, roads_paths, buildings_mask_paths, road_mask_paths):\n    idx_slash_buildings = buildings_paths[0].rfind('/')\n    idx_slash_roads = roads_paths[0].rfind('/')\n    buildings_img_names = list(map(lambda val: val[idx_slash_buildings:], buildings_paths))\n    road_img_names = list(map(lambda val: val[idx_slash_roads:], roads_paths))\n    common_image_paths = set(buildings_img_names).intersection(set(road_img_names))\n    b_train_img_paths, b_train_msk_paths = retrieve_common_images(buildings_paths, common_image_paths, buildings_mask_paths)\n    road_train_img_paths, road_train_mask_paths = retrieve_common_images(buildings_paths, common_image_paths, road_mask_paths)\n\n    return ((b_train_img_paths, b_train_msk_paths), (road_train_img_paths, road_train_mask_paths))\n\n\n\ndef retrieve_common_images(paths, common_paths, path_masks):\n    indexes = []\n    for i, path in enumerate(paths):\n        for com_path in (common_paths):\n            if com_path in path:\n                indexes.append(i)\n    return [paths[i] for i in indexes], [path_masks[i] for i in indexes]\n\ndef get_patches(image, patch_width, patch_heigth, channels):\n    patches = patchify(image, (patch_width, patch_heigth, channels), step=patch_width) #patchify returns an ndarray matrix of images. i.e [3, 3, 1, 500, 500, 3]\n    #thus a reshape is needed, i.e [9, 500, 500, 3] shape as list of images .\n    patches = patches.reshape((patches.shape[0]*patches.shape[1], patch_width, patch_heigth, channels))\n    return patches\n\n\ndef load_images_masks(images_paths, mask_b_paths, mask_r_paths, patch_size, dim, augmentation_fn=None):\n    \"\"\"\n        Load cityscape images and resize\n    \"\"\"\n    set_images = []\n    set_masks = []\n    for img_path, b_mask_path, r_mask_path in zip(images_paths, mask_b_paths, mask_r_paths):\n        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        b_mask = cv2.cvtColor(cv2.imread(b_mask_path), cv2.COLOR_BGR2RGB)\n        b_mask[b_mask==255.] = 1.\n\n        r_mask = cv2.cvtColor(cv2.imread(r_mask_path), cv2.COLOR_BGR2RGB)\n        r_mask[r_mask==255.] = 2.\n\n        #concatenate to form a single mask and then handle in case of overlapping, by assigning to building\n        mask = b_mask + r_mask\n        mask[mask==3.] = 1.\n        \n        #masks = [np.isin(mask, np.array(v)) for v in [[0.], [1.], [2.]]]\n        #mask = np.stack(masks, axis=-1).astype('float')\n\n        img_patches = get_patches(image, patch_size[0], patch_size[1], 3) #[500, 500, 3]\n        mask_patches = get_patches(mask, patch_size[0], patch_size[1], mask.shape[-1])\n        for i in range(img_patches.shape[0]):\n            image_patch = cv2.resize(img_patches[i], (dim[0],dim[1]))\n            mask_patch = cv2.resize(mask_patches[i], (dim[0],dim[1]), interpolation=cv2.INTER_NEAREST_EXACT)\n            if augmentation_fn is not None:\n                result_aug = augmentation_fn(image=image_patch, mask=mask_patch)\n                image_patch = result_aug['image']\n                mask_patch = result_aug['mask']\n            set_images.append(image_patch)\n\n            set_masks.append(mask_patch)\n            \n    return set_images, set_masks    \n\ndef multi_class_mask(masks_input):\n    result = []\n    for idx in range(len(masks_input)):\n        mask = masks_input[idx]\n        masks = [np.isin(mask, np.array(v)) for v in [[0.], [1.], [2.]]]\n        mask = np.stack(masks, axis=-1).astype('float')\n        result.append(mask)\n    return np.array(result)\n\n# Perform colour coding on the reverse-one-hot outputs\n\ngrouped_labels = [[0.],[1.],[2.]]\nmapping_labels = {0: ((0,0,0),[0]), 1: ((255,255,255),[1]), \n                  2: ((255, 0, 0), [2])}\n\nclass Dataset:\n    \"\"\"\n    @param: classes_values: class labels\n    \"\"\"\n    \n    def __init__(\n            self, \n            images_paths, \n            b_msk_paths, \n            r_msk_paths,\n            class_values=grouped_labels,\n            augmentation=None, \n            preprocessing=None,\n            resize=(256,256),\n            doreshape=False\n    ):\n        self.images_fps = images_paths\n        self.b_msk_paths = b_msk_paths\n        self.r_msk_paths = r_msk_paths\n        \n        self.class_values = class_values\n        \n        self.resize=resize\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.doreshape=doreshape\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_fps[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        image = cv2.resize(image, (self.resize[0], self.resize[1]))\n        \n        b_mask = cv2.imread(self.b_msk_paths[i], 0) #0: grey scale mode\n        b_mask = cv2.resize(b_mask, (self.resize[0], self.resize[1]),\n                                  interpolation=cv2.INTER_NEAREST_EXACT).astype('float')\n        b_mask[b_mask==255.] = 1.\n\n        r_mask = cv2.imread(self.r_msk_paths[i], 0) #0: grey scale mode\n        r_mask = cv2.resize(r_mask, (self.resize[0], self.resize[1]),\n                                  interpolation=cv2.INTER_NEAREST_EXACT).astype('float')\n        r_mask[r_mask==255.] = 2.\n        \n        #concatenate to form a single mask and then handle in case of overlapping, by assigning to building\n        mask = b_mask + r_mask\n        mask[mask==3.] = 1.\n        \n        \n        # extract certain classes from mask (e.g. cars)\n        masks = [np.isin(mask, np.array(v)) for v in self.class_values]\n        mask = np.stack(masks, axis=-1).astype('float')\n        \n        # add background if mask is not binary\n        #if mask.shape[-1] != 1:\n        #    background = 1 - mask.sum(axis=-1, keepdims=True)\n        #    mask = np.concatenate((mask, background), axis=-1)\n        \n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing of neural net\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        if self.doreshape == True:\n            mask = np.reshape( mask, (mask.shape[0]*mask.shape[1], mask.shape[2]))\n        return image, mask\n        \n    def __len__(self):\n        return len(self.images_fps)\n    \n    def get_all_data(self):\n        images = []\n        masks = []\n        for i in range(len(self.images_fps)):\n            image, mask = self.__getitem__(i)\n            images.append(image)\n            masks.append(mask)\n        return np.array(images), np.array(masks)\n\n    def convert_to_single_mask(self, mask):\n        new_mask = np.zeros((mask.shape[0], mask.shape[1]))\n        for class_pos in range(mask.shape[2]): \n            tmp = mask[:, :, class_pos].copy()\n            tmp [tmp==1] = class_pos\n            new_mask = new_mask + tmp\n        return new_mask\n    \nclass Dataloder(keras.utils.Sequence):\n    \"\"\"Load data from dataset and form batches\n    \n    Args:\n        dataset: instance of Dataset class for image loading and preprocessing.\n        batch_size: Integet number of images in batch.\n        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n    \"\"\"\n    \n    def __init__(self, dataset, batch_size=1, shuffle=False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataset))\n\n        self.on_epoch_end()\n\n    def __getitem__(self, i):\n        \n        # collect batch data\n        start = i * self.batch_size\n        stop = (i + 1) * self.batch_size\n        data = []\n        for j in range(start, stop):\n            ##PATCHES\n            data.append(self.dataset[j])\n        \n        # transpose list of lists\n        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n        return (batch[0], batch[1])\n    \n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\"\"\"\n        return len(self.indexes) // self.batch_size\n    \n    def on_epoch_end(self):\n        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n        if self.shuffle:\n            self.indexes = np.random.permutation(self.indexes)\n            \ndef get_preprocessing(preprocessing_fn):\n    _transform = [\n        album.Lambda(image=preprocessing_fn),\n    ]\n    return album.Compose(_transform)\n\n\ndef get_training_augmentation(resize):\n    train_transform = [\n\n        album.HorizontalFlip(p=0.5),\n\n        album.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n\n        album.PadIfNeeded(min_width=resize[0], min_height=resize[1], always_apply=True, border_mode=0),\n        album.RandomCrop(width=resize[0], height=resize[1], always_apply=True),\n\n        album.IAAAdditiveGaussianNoise(p=0.2),\n        album.IAAPerspective(p=0.5),\n\n        album.OneOf(\n            [\n                album.CLAHE(p=1),\n                album.RandomBrightness(p=1),\n                album.RandomGamma(p=1),\n            ],\n            p=0.9,\n        ),\n\n        album.OneOf(\n            [\n                album.IAASharpen(p=1),\n                album.Blur(blur_limit=3, p=1),\n                album.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.9,\n        ),\n\n        album.OneOf(\n            [\n                album.RandomContrast(p=1),\n                album.HueSaturationValue(p=1),\n            ],\n            p=0.9,\n        ),\n        album.Lambda(mask=round_clip_0_1)\n    ]\n    return album.Compose(train_transform)\n\ndef round_clip_0_1(x, **kwargs):\n    return x.round().clip(0, 1)","metadata":{"_uuid":"4677bccd-e130-45fa-bd1c-80326aa32224","_cell_guid":"06650728-dfbd-457a-99a5-0d3b483845ff","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}